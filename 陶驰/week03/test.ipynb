{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-31T03:46:35.540740Z",
     "start_time": "2025-10-31T03:46:35.536509Z"
    }
   },
   "source": [
    "# coding:utf8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "from jupyterlab.commands import watch\n",
    "from numpy.array_api import positive\n",
    "\n",
    "from week02.test import correct, epoch\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T07:09:38.263790Z",
     "start_time": "2025-10-31T07:09:38.257791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 任务：尝试在nlpdemo中使用rnn模型训练，判断特定字符在文本中的位置。\n",
    "# 定义一个TorchModel类，固定套路\n",
    "class TorchModel(nn.Module):\n",
    "    def __init__(self, vector_dim, sentence_length, vocab):  # 接收词向量维度、句子长度、词表\n",
    "        # 首先一步是调用父类构造器,这一行也得背记下来\n",
    "        super(TorchModel, self).__init__()\n",
    "        # 构建embedding层，nn.Embedding接收几个参数：词表大小、词向量维度、用什么编号作为填充\n",
    "        self.embedding = nn.Embedding(len(vocab), vector_dim, padding_idx=0)\n",
    "        # rnn层，输入层大小vector_dim,输出层大小vector_dim,输入和输出的张量形状一battch,seq_len,dim为主流形式\n",
    "        self.rnn = nn.RNN(vector_dim, vector_dim, batch_first=True)\n",
    "        # 线性层，把每个时间步的RNN输出映射为一个标量（用于二分类）\n",
    "        self.classify = nn.Linear(vector_dim, 1)\n",
    "        # 激活函数，用sigmoid函数把标量映射到[0,1]概率\n",
    "        self.activation = torch.sigmoid\n",
    "        # 损失函数，其实这里使用最好的二分类损失应该使用BCE\n",
    "        self.loss = nn.functional.mse_loss\n",
    "\n",
    "    # 正向传播，x是输入，y为可选，如果给了真实标签就返回损失用于训练\n",
    "    def forward(self, x, y=None):\n",
    "        # 把编号序列转换为向量序列，形状由（batch,seq_len）->(batch, seq_len, vector_dim)\n",
    "        x = self.embedding(x)\n",
    "        #  把embedding传给RNN，rnn_out是每个时间步的输出，第二个返回值是隐藏状态，这里并没有使用\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        # 把每一个时间步的输出映射到一个书logit，形状(batch,seq_len,1)\n",
    "        y_pred = self.classify(rnn_out)\n",
    "        # 如果传入真实标签则返回损失，否则返回预测值\n",
    "        if y is not None:\n",
    "            return self.loss(y_pred,y)\n",
    "        else:\n",
    "            return y_pred\n"
   ],
   "id": "bcf1971b36619537",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T07:20:09.388106Z",
     "start_time": "2025-10-31T07:20:09.381912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 词表构造函数\n",
    "def build_vocab():\n",
    "    chars = \"你我他defghijklmnopqrstuvwxyz\"  # 定义字符集\n",
    "    vocab = {\"pad\": 0}  # vocab初始用0作为填充\n",
    "    for index, char in enumerate(chars):\n",
    "        vocab[char] = index + 1  # 每个字对应一个序号\n",
    "    vocab['unk'] = len(vocab)  # 未知字符放在最后一位\n",
    "    return vocab\n",
    "\n",
    "build_vocab()"
   ],
   "id": "b7f1c1e3b4fe4301",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pad': 0,\n",
       " '你': 1,\n",
       " '我': 2,\n",
       " '他': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " 'unk': 27}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:14:35.898888Z",
     "start_time": "2025-10-31T08:14:35.891596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 随机生产单个样本\n",
    "def build_sample(vocab, sentence_length):\n",
    "    # 随机从词表里面选择sentence_length个字符，可能重复，生产一个字符序列\n",
    "    x = [random.choice(list(vocab.keys())) for _ in range(sentence_length)]\n",
    "    y = []\n",
    "    for char in x:\n",
    "        if char in set(\"你我他\"):\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    x = [vocab.get(word, vocab['unk']) for word in x]\n",
    "    return x,y\n",
    "\n",
    "build_sample(build_vocab(),7)"
   ],
   "id": "cd8f1eff59aeb349",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([14, 3, 2, 8, 9, 4, 3], [0, 1, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:33:33.882845Z",
     "start_time": "2025-10-31T08:33:33.876848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 构建批量数据集\n",
    "def build_dataset(sample_length, vocab, sentence_length):\n",
    "    dataset_x = []\n",
    "    dataset_y = []\n",
    "    for i in range(sample_length):\n",
    "        x, y = build_sample(vocab, sentence_length)\n",
    "        dataset_x.append(x)\n",
    "        dataset_y.append(y)\n",
    "    # 把python列表转换为torch的张量：LongTensor用于索引，FloatTensor用于标签\n",
    "    # unsqueeze把标签形状从(batch, seq_len)变成(batch,seq,len,1)\n",
    "    return torch.LongTensor(dataset_x), torch.FloatTensor(dataset_y).unsqueeze(-1)\n",
    "build_dataset(2,build_vocab(),7)"
   ],
   "id": "fa8cf27a7005d189",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[14, 25, 21, 17,  2,  3, 23],\n",
       "         [ 8, 23, 15,  7, 26,  9, 22]]),\n",
       " tensor([[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [1.],\n",
       "          [1.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:35:00.813388Z",
     "start_time": "2025-10-31T08:35:00.809369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 构建包装函数\n",
    "def build_model(vocab, char_dim, sentence_length):\n",
    "    model = TorchModel(char_dim, sentence_length, vocab)\n",
    "    return model"
   ],
   "id": "1638acc7acdc0eb3",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:51:26.421268Z",
     "start_time": "2025-10-31T08:51:26.415229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 评估函数\n",
    "def evaluate(model, vocab, sample_length):\n",
    "    model.eval()  # 切换到评估模式，背下来\n",
    "    # 生成200个样本作为测试集\n",
    "    x, y = build_dataset(200, vocab, sample_length)\n",
    "\n",
    "    # 统计正样本个数\n",
    "    positive_positions = int(y.sum())\n",
    "    # 统计总样本数\n",
    "    total_positions = 200 * sample_length\n",
    "    print(\"本次预测集中共有%d个正样本位置,%d个负样本位置\" %\n",
    "          (positive_positions, total_positions - positive_positions))\n",
    "\n",
    "    correct, wrong = 0, 0\n",
    "    # 关闭梯度计算\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x)\n",
    "\n",
    "        for y_p_seq,y_t_seq in zip(y_pred,y):\n",
    "            for y_p, y_t in zip(y_p_seq, y_t_seq):\n",
    "                if float(y_p) < 0.5 and int(y_t) == 0:\n",
    "                    correct += 1\n",
    "                elif float(y_p) >= 0.5 and int(y_t) == 1:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    wrong += 1\n",
    "    print(\"正确预测个数:%d, 正确率:%f\" % (correct, correct / (correct + wrong)))\n",
    "    return correct / (correct + wrong)"
   ],
   "id": "894cd1cc5a688ccc",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:51:26.847669Z",
     "start_time": "2025-10-31T08:51:26.841094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # 配置参数\n",
    "    epoch_num = 20 # 训练轮数\n",
    "    batch_size = 20  # 每次训练的样本个数\n",
    "    train_sample = 500  # 每轮训练总共训练的样本总数\n",
    "    char_dim = 20  # 每个字的维度\n",
    "    sentence_length = 6  # 句子长度\n",
    "    learning_rate = 0.005 # 学习率\n",
    "    # 建立词表\n",
    "    vocab = build_vocab()\n",
    "    # 建立模型\n",
    "    model = build_model(vocab, char_dim, sentence_length)\n",
    "    # 选择优化器\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    log = []\n",
    "    # 训练过程,一轮一轮训练\n",
    "    for epoch in range(epoch_num):\n",
    "        model.train()\n",
    "        watch_loss = []\n",
    "        for batch in range(int(train_sample / batch_size)):\n",
    "            x, y = build_dataset(batch_size, vocab, sentence_length) # 构造一个数据集\n",
    "            optim.zero_grad() # 梯度归零\n",
    "            loss = model(x, y)\n",
    "            loss.backward() # 计算梯度\n",
    "            optim.step() # 更新权重\n",
    "\n",
    "            watch_loss.append(loss.item())\n",
    "        print(\"=========\\n第%d轮平均loss:%f\" % (epoch + 1, np.mean(watch_loss)))\n",
    "        acc = evaluate(model, vocab, sentence_length)  # 测试本轮模型结果\n",
    "        log.append([acc, np.mean(watch_loss)])\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), \"model.pth\")\n",
    "    # 保存词表\n",
    "    writer = open(\"vocab.json\", \"w\", encoding=\"utf8\")\n",
    "    writer.write(json.dumps(vocab, ensure_ascii=False, indent=2))\n",
    "    writer.close()\n",
    "    return\n"
   ],
   "id": "b2c42c6229277323",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:51:27.390323Z",
     "start_time": "2025-10-31T08:51:27.384311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(model_path, vocab_path, input_strings):\n",
    "    char_dim = 20  # 每个字的维度\n",
    "    sentence_length = 6  # 样本文本长度\n",
    "    vocab = json.load(open(vocab_path, \"r\", encoding=\"utf8\"))  # 加载字符表\n",
    "    model = build_model(vocab, char_dim, sentence_length)  # 建立模型\n",
    "    model.load_state_dict(torch.load(model_path))  # 加载训练好的权重\n",
    "\n",
    "    x = []\n",
    "    for input_string in input_strings:\n",
    "        x.append([vocab.get(char, vocab['unk']) for char in input_string])  # 将输入序列化\n",
    "\n",
    "    model.eval()  # 测试模式\n",
    "    with torch.no_grad():  # 不计算梯度\n",
    "        result = model.forward(torch.LongTensor(x))  # 模型预测 (batch_size, sen_len, 1)\n",
    "        result = result.squeeze(-1)  # (batch_size, sen_len)\n",
    "\n",
    "    for i, input_string in enumerate(input_strings):\n",
    "        print(f\"\\n输入:{input_string}\")\n",
    "        print(\"=\" * 50)\n",
    "        for j, char in enumerate(input_string):\n",
    "            prob = float(result[i][j])\n",
    "            is_target = \"是\" if prob >= 0.5 else \"否\"\n",
    "            print(f\"  位置{j} 字符'{char}': {is_target}特定字符 (概率:{prob:.4f})\")\n"
   ],
   "id": "a738dfd8ee46e6e1",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:51:29.363573Z",
     "start_time": "2025-10-31T08:51:28.195773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    test_strings = [\"fnvf我e\", \"wz你dfg\", \"rqwdeg\", \"n我k他ww\"]\n",
    "    predict(\"model.pth\", \"vocab.json\", test_strings)\n"
   ],
   "id": "4a0968f51f36386d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========\n",
      "第1轮平均loss:0.045528\n",
      "本次预测集中共有123个正样本位置,1077个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第2轮平均loss:0.003898\n",
      "本次预测集中共有119个正样本位置,1081个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第3轮平均loss:0.000993\n",
      "本次预测集中共有143个正样本位置,1057个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第4轮平均loss:0.000487\n",
      "本次预测集中共有128个正样本位置,1072个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第5轮平均loss:0.000315\n",
      "本次预测集中共有119个正样本位置,1081个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第6轮平均loss:0.000243\n",
      "本次预测集中共有106个正样本位置,1094个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第7轮平均loss:0.000180\n",
      "本次预测集中共有118个正样本位置,1082个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第8轮平均loss:0.000159\n",
      "本次预测集中共有129个正样本位置,1071个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第9轮平均loss:0.000121\n",
      "本次预测集中共有121个正样本位置,1079个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第10轮平均loss:0.000108\n",
      "本次预测集中共有148个正样本位置,1052个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第11轮平均loss:0.000091\n",
      "本次预测集中共有139个正样本位置,1061个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第12轮平均loss:0.000077\n",
      "本次预测集中共有128个正样本位置,1072个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第13轮平均loss:0.000071\n",
      "本次预测集中共有138个正样本位置,1062个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第14轮平均loss:0.000069\n",
      "本次预测集中共有121个正样本位置,1079个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第15轮平均loss:0.000061\n",
      "本次预测集中共有151个正样本位置,1049个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第16轮平均loss:0.000052\n",
      "本次预测集中共有127个正样本位置,1073个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第17轮平均loss:0.000050\n",
      "本次预测集中共有117个正样本位置,1083个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第18轮平均loss:0.000039\n",
      "本次预测集中共有133个正样本位置,1067个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第19轮平均loss:0.000038\n",
      "本次预测集中共有126个正样本位置,1074个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "=========\n",
      "第20轮平均loss:0.000034\n",
      "本次预测集中共有110个正样本位置,1090个负样本位置\n",
      "正确预测个数:1200, 正确率:1.000000\n",
      "\n",
      "输入:fnvf我e\n",
      "==================================================\n",
      "  位置0 字符'f': 否特定字符 (概率:0.0065)\n",
      "  位置1 字符'n': 否特定字符 (概率:0.0018)\n",
      "  位置2 字符'v': 否特定字符 (概率:0.0103)\n",
      "  位置3 字符'f': 否特定字符 (概率:0.0016)\n",
      "  位置4 字符'我': 是特定字符 (概率:1.0152)\n",
      "  位置5 字符'e': 否特定字符 (概率:-0.0076)\n",
      "\n",
      "输入:wz你dfg\n",
      "==================================================\n",
      "  位置0 字符'w': 否特定字符 (概率:0.0025)\n",
      "  位置1 字符'z': 否特定字符 (概率:0.0130)\n",
      "  位置2 字符'你': 是特定字符 (概率:0.9937)\n",
      "  位置3 字符'd': 否特定字符 (概率:0.0046)\n",
      "  位置4 字符'f': 否特定字符 (概率:0.0047)\n",
      "  位置5 字符'g': 否特定字符 (概率:-0.0093)\n",
      "\n",
      "输入:rqwdeg\n",
      "==================================================\n",
      "  位置0 字符'r': 否特定字符 (概率:0.0047)\n",
      "  位置1 字符'q': 否特定字符 (概率:0.0023)\n",
      "  位置2 字符'w': 否特定字符 (概率:0.0067)\n",
      "  位置3 字符'd': 否特定字符 (概率:0.0098)\n",
      "  位置4 字符'e': 否特定字符 (概率:-0.0035)\n",
      "  位置5 字符'g': 否特定字符 (概率:-0.0031)\n",
      "\n",
      "输入:n我k他ww\n",
      "==================================================\n",
      "  位置0 字符'n': 否特定字符 (概率:0.0033)\n",
      "  位置1 字符'我': 是特定字符 (概率:1.0039)\n",
      "  位置2 字符'k': 否特定字符 (概率:-0.0008)\n",
      "  位置3 字符'他': 是特定字符 (概率:1.0106)\n",
      "  位置4 字符'w': 否特定字符 (概率:0.0065)\n",
      "  位置5 字符'w': 否特定字符 (概率:0.0070)\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "279f049b4d87b22e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
